## REVE-CE: Remote Embodied Visual Referring Expression in Continuous Environment

Enabling robotics to execute actions in visual world via natural language instructions is a consistent goal and challenge. Recently, several datasets are released to solve this challenge including Vision Language Navigation (VLN) and Remote Embodied Visual Referring Expression in Real Indoor Environments (REVERIE). Compared to VLN, REVERIE is closer with the original intention of this challenge since its has higher guidance level instructions and referring target object for each trajectory. However, the navigation process of REVERIE is based on a navigation graph provided by the discrete environment, which is unrealistic in the unseen scenes in real world. To make REVERIE task more consistent with the physical world, we develop a remote embodied visual referring expression task in a continuous 3D environment in which agent is required to execute a munch longer sequence of low-level actions under the guidance of language instructions, namely REVE-CE. We provide a suite of baselines transferred from the state-of-art works in VLN and find that they could not perform well on REVE-CE. Further, we propose a multi-branch cross modal attention (MBCMA) framework and test it on REVE-CE, and our proposed framework outperforms existing baselines on REVE-CE over all key metrics and set a new benchmark for REVE-CE.

<video id="video" controls="controls" preload="none" width="1280" height="538" poster='illustration_v3.png'>
          <source id="mp4" src="illustration_v3.m4v" type="video/mp4">
          <p>Your user agent does not support the HTML5 Video element.</p>
</video>
